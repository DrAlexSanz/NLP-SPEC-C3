{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copia de Untitled5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPBn2/xPYs4LiDVnoDeeIKV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DrAlexSanz/NLP-SPEC-C3/blob/main/W1/Assignment_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nheRiEhsQ6xO"
      },
      "source": [
        "# Assignment 1: Sentiment with Deep Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5vJmkBSROld"
      },
      "source": [
        "In course 1, you implemented Logistic regression and Naive Bayes for sentiment analysis. However if you were to give your old models an example like:\n",
        "\n",
        "Your model would have predicted a positive sentiment for that review. However, that sentence has a negative sentiment and indicates that the movie was not good. To solve those kinds of misclassifications, you will write a program that uses deep neural networks to identify sentiment in text. By completing this assignment, you will:\n",
        "\n",
        "* Understand how you can build/design a model using layers\n",
        "* Train a model using a training loop\n",
        "* Use a binary cross-entropy loss function\n",
        "* Compute the accuracy of your model\n",
        "* Predict using your own input\n",
        "\n",
        "As you can tell, this model follows a similar structure to the one you previously implemented in the second course of this specialization.\n",
        "\n",
        "Indeed most of the deep nets you will be implementing will have a similar structure. The only thing that changes is the model architecture, the inputs, and the outputs. Before starting the assignment, we will introduce you to the Google library trax that we use for building and training models.\n",
        "Now we will show you how to compute the gradient of a certain function f by just using .grad(f).\n",
        "\n",
        "Trax source code can be found on Github: Trax\n",
        "The Trax code also uses the JAX library: JAX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FwigBZrRZ_4"
      },
      "source": [
        "## Part 1: Import libraries and try out Trax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZHfL6lDRbqG",
        "outputId": "4526a1f6-400f-40e3-80ef-672a0e955eae"
      },
      "source": [
        "import os \n",
        "import random as rnd\n",
        "\n",
        "# Install trax\n",
        "\n",
        "!pip install sentencepiece==0.1.91\n",
        "!pip install trax\n",
        "\n",
        "# import relevant libraries\n",
        "import trax\n",
        "\n",
        "# set random seeds to make this notebook easier to replicate\n",
        "#trax.supervised.trainer_lib.init_random_number_generators(31)\n",
        "\n",
        "# import trax.fastmath.numpy\n",
        "import trax.fastmath.numpy as np\n",
        "\n",
        "# import trax.layers\n",
        "from trax import layers as tl\n",
        "\n",
        "# Download the utils file\n",
        "\n",
        "!wget https://raw.githubusercontent.com/DrAlexSanz/NLP-SPEC-C3/main/W1/utils.py\n",
        "\n",
        "# import Layer from the utils.py file\n",
        "from utils import Layer, load_tweets, process_tweet\n",
        "\n",
        "print(\"Imports OK\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 7.9MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.91\n",
            "Collecting trax\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/1d/c0a3aeed127c26a0c3f0925fc9cc7278c272e52310eedfc322477e854972/trax-1.3.6-py2.py3-none-any.whl (468kB)\n",
            "\u001b[K     |████████████████████████████████| 471kB 10.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: jax in /usr/local/lib/python3.6/dist-packages (from trax) (0.2.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from trax) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from trax) (1.18.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from trax) (0.10.0)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.6/dist-packages (from trax) (0.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from trax) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.6/dist-packages (from trax) (4.0.1)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from trax) (0.17.3)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.6/dist-packages (from trax) (0.1.56+cuda101)\n",
            "Collecting tensorflow-text\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/b2/2dbd90b93913afd07e6101b8b84327c401c394e60141c1e98590038060b3/tensorflow_text-2.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6MB 17.1MB/s \n",
            "\u001b[?25hCollecting funcsigs\n",
            "  Downloading https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
            "Collecting t5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/39/a607d2450190af7675e4f77c5eff0cc9a83f82fe63fb396872ef2004106b/t5-0.7.1-py3-none-any.whl (172kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 42.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum in /usr/local/lib/python3.6/dist-packages (from jax->trax) (3.3.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (2.23.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (2.3)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (1.1.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (0.7)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (3.12.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (4.41.1)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (20.2.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (0.3.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (0.16.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (0.24.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (0.1.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->trax) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->trax) (1.5.0)\n",
            "Requirement already satisfied: tensorflow<2.4,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-text->trax) (2.3.0)\n",
            "Collecting transformers>=2.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 35.5MB/s \n",
            "\u001b[?25hCollecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/c4/8e948f601a4f9609e8b2b58f31966cb13cf17b940b82aa3e767f01c42c52/sacrebleu-1.4.14-py3-none-any.whl (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from t5->trax) (0.1.91)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from t5->trax) (1.1.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from t5->trax) (0.22.2.post1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from t5->trax) (3.2.5)\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.6/dist-packages (from t5->trax) (2.8.0)\n",
            "Collecting tfds-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/d8/2afc9bea33c7cca8d051e864ee43df5414e0611a166ba3ddea4ed7e5315f/tfds_nightly-4.1.0.dev202011230107-py3-none-any.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 36.3MB/s \n",
            "\u001b[?25hCollecting mesh-tensorflow[transformer]>=0.1.13\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/8b/553deb763ce8d00afb17debab7cb14a87b209cd4c6f0e8ecfc8d884cb12a/mesh_tensorflow-0.1.17-py3-none-any.whl (342kB)\n",
            "\u001b[K     |████████████████████████████████| 348kB 37.1MB/s \n",
            "\u001b[?25hCollecting rouge-score\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from t5->trax) (1.7.0+cu101)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2020.6.20)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow-datasets->trax) (3.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-datasets->trax) (50.3.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow-datasets->trax) (1.52.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (2.3.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.6.3)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (2.3.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.3.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.2.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.33.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.12.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.35.1)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (2.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5->trax) (3.0.12)\n",
            "Collecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 18.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5->trax) (20.4)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 34.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5->trax) (2019.12.20)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->t5->trax) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->t5->trax) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->t5->trax) (0.17.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5->trax) (3.7.4.3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.17.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.7.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (3.3.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.4.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers>=2.7.0->t5->trax) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.7.0->t5->trax) (7.1.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (4.1.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.2.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (2.0.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (3.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=4b03d46b79ff816cb07439c18e669ff79d62da2def66b965905aa2b1c5a6a8a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tensorflow-text, funcsigs, tokenizers, sacremoses, transformers, portalocker, sacrebleu, tfds-nightly, mesh-tensorflow, rouge-score, t5, trax\n",
            "Successfully installed funcsigs-1.0.2 mesh-tensorflow-0.1.17 portalocker-2.0.0 rouge-score-0.0.4 sacrebleu-1.4.14 sacremoses-0.0.43 t5-0.7.1 tensorflow-text-2.3.0 tfds-nightly-4.1.0.dev202011230107 tokenizers-0.9.3 transformers-3.5.1 trax-1.3.6\n",
            "--2020-11-23 10:33:00--  https://raw.githubusercontent.com/DrAlexSanz/NLP-SPEC-C3/main/W1/utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2789 (2.7K) [text/plain]\n",
            "Saving to: ‘utils.py’\n",
            "\n",
            "utils.py            100%[===================>]   2.72K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-11-23 10:33:01 (24.6 MB/s) - ‘utils.py’ saved [2789/2789]\n",
            "\n",
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Imports OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeUgtqZ5i1fw"
      },
      "source": [
        "### Since I imported trax's version of numpy I can create vectors directly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3E4lMO1MjGIi",
        "outputId": "3a3a7b14-0d96-4f1e-92cd-8114548c5063"
      },
      "source": [
        "a = np.array((5., 2.))\n",
        "\n",
        "type(a) # Notice it's not a np array but a jax DeviceArray."
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "jax.interpreters.xla.DeviceArray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaDTH7ohjd4T",
        "outputId": "522002dc-3847-4ed8-ea08-0961a132fe75"
      },
      "source": [
        "# Now do a function with the same array\n",
        "\n",
        "def f(x):\n",
        "\n",
        "    return(x**2)\n",
        "\n",
        "print(f\"f(a) = {f(a)}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "f(a) = [25.  4.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdWRmkH3lV1J",
        "outputId": "c00bc467-4530-4aac-fe19-80ee28f84678"
      },
      "source": [
        "# And now the derivative (2x)\n",
        "\n",
        "grad_f = trax.fastmath.grad(fun = f) # grad only takes scalar arguments\n",
        "type(grad_f)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "function"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "GhxAa0mcljy6",
        "outputId": "9e8c5708-6898-4097-c45b-16a84e19e1b6"
      },
      "source": [
        "# grad_f(a)\n",
        "b = 13.0\n",
        "grad_b = grad_f(b)\n",
        "\n",
        "display(grad_b)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DeviceArray(26., dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVb6aFN4mNI2"
      },
      "source": [
        "# Part 2: Importing the data\n",
        "\n",
        "## 2.1 Loading in the data\n",
        "Import the data set.\n",
        "\n",
        "* You may recognize this from earlier assignments in the specialization.\n",
        "* Details of process_tweet function are available in utils.py file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lkv1y-RKmTVN"
      },
      "source": [
        "import numpy as np # Let's go back to the usual thing."
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONT7qtvEmcgk",
        "outputId": "ec1cb0b3-aa7b-42d5-f627-dda377408e5e"
      },
      "source": [
        "all_pos_tweets, all_neg_tweets = load_tweets()\n",
        "\n",
        "print(\"Number of positive tweets\", len(all_pos_tweets))\n",
        "print(\"Number of negative tweets\", len(all_neg_tweets))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of positive tweets 5000\n",
            "Number of negative tweets 5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x_VX39lno33"
      },
      "source": [
        "## Now I'll create train and validation sets\n",
        "\n",
        "* Shuffle the tweets if they are not randomly sorted.\n",
        "* Split the positive in train-test (80-20 because I can).\n",
        "* Add labels (1 positive, 0 negative).\n",
        "* Check it all."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-C3PiPFn_Aa",
        "outputId": "423acf11-3cc0-486d-e41d-e7fdd94bd83d"
      },
      "source": [
        "train_pos = all_pos_tweets[:4000]\n",
        "test_pos = all_pos_tweets[4000:]\n",
        "\n",
        "train_neg = all_neg_tweets[:4000]\n",
        "test_neg = all_neg_tweets[4000:]\n",
        "\n",
        "train_x = train_pos + train_neg\n",
        "test_x = test_pos + test_neg\n",
        "\n",
        "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
        "\n",
        "print(\"Length of train_pos:\", len(train_pos))\n",
        "print(\"Length of train_neg:\", len(train_neg))\n",
        "\n",
        "print(\"Length of test_pos:\", len(test_pos))\n",
        "print(\"Length of test_neg:\", len(test_neg))\n",
        "\n",
        "print(\"Length of train_x:\", len(train_x))\n",
        "print(\"Length of train_y:\", len(train_y))\n",
        "\n",
        "print(\"First 5 values of tags\", train_y[0:5])\n",
        "print(\"Last 5 values of tags\", train_y[-5:])\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of train_pos: 4000\n",
            "Length of train_neg: 4000\n",
            "Length of test_pos: 1000\n",
            "Length of test_neg: 1000\n",
            "Length of train_x: 8000\n",
            "Length of train_y: 8000\n",
            "First 5 values of tags [1. 1. 1. 1. 1.]\n",
            "Last 5 values of tags [0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLJsTGK_qid-"
      },
      "source": [
        "### Preprocess the tweets to clean them. I have a function but in any case I'm used to this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehLfODQrqrT_",
        "outputId": "867b3a77-88e4-4fa5-bd55-43b2d64248e9"
      },
      "source": [
        "# This function only processes one tweet. I'll call it in a loop or a list comprehension\n",
        "\n",
        "print(\"The first positive tweet is:\", all_pos_tweets[0])\n",
        "\n",
        "clean_tweet = process_tweet(all_pos_tweets[0])\n",
        "\n",
        "print(\"The clean tweet is:\", clean_tweet) # Notice it removes all the twitter handles and the \"#\" symbol. It also tokenizes and stems the words."
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The first positive tweet is: #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
            "The clean tweet is: ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NrQh_qYrqfo"
      },
      "source": [
        "# 2.2 Building the vocabulary\n",
        "Now build the vocabulary.\n",
        "\n",
        "* Map each word in each tweet to an integer (an \"index\").\n",
        "* The following code does this for you, but please read it and understand what it's doing.\n",
        "* Note that you will build the vocabulary based on the training data.\n",
        "* To do so, you will assign an index to everyword by iterating over your training set.\n",
        "* The vocabulary will also include some special tokens\n",
        "\n",
        "* <--PAD--> padding\n",
        "* <--END-->: end of line\n",
        "* <--UNK-->: a token representing any word that is not in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXor3QX7vL1L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8848100-3424-4e18-ca2c-f56f31a0739e"
      },
      "source": [
        "# Start with the padding, end and UNK\n",
        "\n",
        "vocab = {\"<PAD>\": 0, \"<END>\": 1, \"<UNK>\": 2}\n",
        "\n",
        "# Keep in mind, the vocabulary is only with the training data!!\n",
        "\n",
        "for tweet in train_x:\n",
        "    processed_tweet = process_tweet(tweet)\n",
        "\n",
        "    for word in processed_tweet:\n",
        "        if word not in vocab:\n",
        "            vocab[word] = len(vocab) # len vocab changes with every new word\n",
        "\n",
        "print(\"Total words:\", len(vocab))\n",
        "\n",
        "#display(vocab)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total words: 9092\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM2UNjeh2NLx"
      },
      "source": [
        "## Exercise 01\n",
        "Instructions: Write a program tweet_to_tensor that takes in a tweet and converts it to an array of numbers. You can use the Vocab dictionary you just found to help create the tensor.\n",
        "\n",
        "* Use the vocab_dict parameter and not a global variable.\n",
        "* Do not hard code the integer value for the __UNK__ token.\n",
        "* Map each word in tweet to corresponding token in 'Vocab'\n",
        "* Use Python's Dictionary.get(key,value) so that the function returns a default value if the key is not found in the dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2s9d7XQ2eLX"
      },
      "source": [
        "def tweet_to_tensor(tweet, vocab_dict, unk_token = \"<UNK>\"):\n",
        "    \"\"\"\n",
        "    Take a tweet (tokens) and return a tensor (numbers) basically, translate from keys to values\n",
        "    Inputs:\n",
        "      Tweet: a clean tweet\n",
        "      vocab_dict: the vocabulary, word and index\n",
        "    Output:\n",
        "      tensor_l: a vector (list) with the indices of the words\n",
        "    \"\"\"\n",
        "    word_list = process_tweet(tweet)\n",
        "\n",
        "    unk_ID = vocab_dict[unk_token]\n",
        "    tensor_l = []\n",
        "\n",
        "    for word in word_list:\n",
        "      if word in vocab_dict:\n",
        "        word_ID = vocab_dict[word]\n",
        "        tensor_l.append(word_ID)\n",
        "      else:\n",
        "        tensor_l.append(unk_ID)\n",
        "\n",
        "    return tensor_l\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAs0CoVb4HPL",
        "outputId": "1650cf0f-9151-4716-a7e7-e8269ad1c8d7"
      },
      "source": [
        "print(\"Tweet is:\", test_pos[0])\n",
        "tensor_tweet = tweet_to_tensor(test_pos[0], vocab)\n",
        "print(\"Corresponding tensor is:\", tensor_tweet)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tweet is: Bro:U wan cut hair anot,ur hair long Liao bo\n",
            "Me:since ord liao,take it easy lor treat as save $ leave it longer :)\n",
            "Bro:LOL Sibei xialan\n",
            "Corresponding tensor is: [1065, 136, 479, 2351, 745, 8146, 1123, 745, 53, 2, 2672, 791, 2, 2, 349, 601, 2, 3489, 1017, 597, 4559, 9, 1065, 157, 2, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41fpcfuQ43GL",
        "outputId": "ea70a587-b536-4189-97ae-653c4abd352c"
      },
      "source": [
        "#Thorough checks\n",
        "\n",
        "# test tweet_to_tensor\n",
        "\n",
        "def test_tweet_to_tensor():\n",
        "    test_cases = [\n",
        "        \n",
        "        {\n",
        "            \"name\":\"simple_test_check\",\n",
        "            \"input\": [test_pos[1], vocab],\n",
        "            \"expected\":[444, 2, 304, 567, 56, 9],\n",
        "            \"error\":\"The function gives bad output for test_pos[1]. Test failed\"\n",
        "        },\n",
        "        {\n",
        "            \"name\":\"datatype_check\",\n",
        "            \"input\":[test_pos[1], vocab],\n",
        "            \"expected\":type([]),\n",
        "            \"error\":\"Datatype mismatch. Need only list not np.array\"\n",
        "        },\n",
        "        {\n",
        "            \"name\":\"without_unk_check\",\n",
        "            \"input\":[test_pos[1], vocab],\n",
        "            \"expected\":6,\n",
        "            \"error\":\"Unk word check not done- Please check if you included mapping for unknown word\"\n",
        "        }\n",
        "    ]\n",
        "    count = 0\n",
        "    for test_case in test_cases:\n",
        "        \n",
        "        try:\n",
        "            if test_case['name'] == \"simple_test_check\":\n",
        "                assert test_case[\"expected\"] == tweet_to_tensor(*test_case['input'])\n",
        "                count += 1\n",
        "            if test_case['name'] == \"datatype_check\":\n",
        "                assert isinstance(tweet_to_tensor(*test_case['input']), test_case[\"expected\"])\n",
        "                count += 1\n",
        "            if test_case['name'] == \"without_unk_check\":\n",
        "                assert None not in tweet_to_tensor(*test_case['input'])\n",
        "                count += 1\n",
        "                \n",
        "            \n",
        "            \n",
        "        except:\n",
        "            print(test_case['error'])\n",
        "    if count == 3:\n",
        "        print(\"\\033[92m All tests passed\")\n",
        "    else:\n",
        "        print(count,\" Tests passed out of 3\")\n",
        "test_tweet_to_tensor()\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[92m All tests passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPhhWyHfEIHt"
      },
      "source": [
        "## 2.4 Creating a batch generator\n",
        "Most of the time in Natural Language Processing, and AI in general we use batches when training our data sets.\n",
        "\n",
        "If instead of training with batches of examples, you were to train a model with one example at a time, it would take a very long time to train the model.\n",
        "\n",
        "* You will now build a data generator that takes in the positive/negative tweets and returns a batch of training examples.\n",
        "* It returns the model inputs, the targets (positive or negative labels) and the weight for each target (ex: this allows us to can treat some examples as more important to get right than others, but commonly this will all be 1.0).\n",
        "* Once you create the generator, you could include it in a for loop.\n",
        "\n",
        "The generator returns the next batch each time it's called.\n",
        "\n",
        "* This generator returns the data in a format (tensors) that you could directly use in your model.\n",
        "It returns a triplet: the inputs, targets, and loss weights:\n",
        "* Inputs is a tensor that contains the batch of tweets we put into the model.\n",
        "* Targets is the corresponding batch of labels that we train to generate.\n",
        "* Loss weights here are just 1s with same shape as targets.\n",
        "\n",
        "Next week, you will use it to mask input padding.\n",
        "\n",
        "###Exercise 02\n",
        "Implement data_generator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEEQAfuREw_M"
      },
      "source": [
        "def data_generator(data_pos, data_neg, batch_size, loop, vocab_dict, shuffle = False):\n",
        "    \"\"\"\n",
        "    Take the tweets and return batches\n",
        "    Inputs:\n",
        "      data_pos/data_neg: Tweets that I want to group in batches\n",
        "      batch_size\n",
        "      loop: If False, return one batch only\n",
        "      vocab_dict: THe list of unique words\n",
        "      Shuffle: Boolean to decide wether to shuffle the tweets or not.\n",
        "    Outputs:\n",
        "      Tensors: batch of positive and negative tweets\n",
        "      Labels: associated labels\n",
        "      example_weights: An array that specifies the importance of each tweet    \n",
        "    \"\"\"\n",
        "    # I want an even number of tweets to have 50-50 split between positive and negative\n",
        "    assert batch_size % 2 == 0\n",
        "\n",
        "    # Number of tweets from each class\n",
        "    n_to_take = batch_size // 2\n",
        "\n",
        "    # Start at the beginning\n",
        "    pos_index = 0\n",
        "    neg_index = 0\n",
        "\n",
        "    # Limit at the end\n",
        "\n",
        "    len_data_pos = len(data_pos)\n",
        "    len_data_neg = len(data_neg)\n",
        "\n",
        "    pos_index_list = list(range(len_data_pos))\n",
        "    neg_index_list = list(range(len_data_neg))\n",
        "\n",
        "    # Shuffle if shuffling is enabled. Only need to shuffle the indices\n",
        "    if shuffle:\n",
        "        rnd.shuffle(pos_index_list)\n",
        "        rnd.shuffle(neg_index_list)\n",
        "\n",
        "    stop = False # Set it as a normally closed switch\n",
        "\n",
        "    while not stop:\n",
        "        batch = []\n",
        "\n",
        "        # get n_to_take positive examples\n",
        "\n",
        "        for i in range(n_to_take):\n",
        "            if pos_index >= len_data_pos:\n",
        "\n",
        "                if not loop:\n",
        "                    stop = True\n",
        "                    break;\n",
        "                    # If it's only one example, do only one\n",
        "                \n",
        "            # To keep reusing the data, reset the index\n",
        "            pos_index = 0\n",
        "            if shuffle:\n",
        "                rnd.shuffle(pos_index_list) # shuffle again\n",
        "            \n",
        "            #Get the tweets\n",
        "            tweet = data_pos[pos_index_list[pos_index]] # Beware of the slicing, pos_index list is a list!\n",
        "            tensor = tweet_to_tensor(tweet, vocab_dict)\n",
        "            batch.append(tensor)\n",
        "            pos_index += 1\n",
        "        \n",
        "        # Now the negative tweets\n",
        "\n",
        "        for i in range(n_to_take):\n",
        "            if neg_index >= len_data_neg:\n",
        "\n",
        "                if not loop:\n",
        "                    stop = True\n",
        "                    break;\n",
        "\n",
        "            neg_index = 0\n",
        "            if shuffle:\n",
        "                rnd.shuffle(neg_index_list)\n",
        "\n",
        "            tweet = data_neg[neg_index_list[neg_index]]\n",
        "            tensor = tweet_to_tensor(tweet, vocab_dict)\n",
        "            batch.append(tensor)\n",
        "            neg_index += 1\n",
        "\n",
        "      \n",
        "      # Second part. Now that I have the tweets in a batch, PAD them.\n",
        "      # I could also do it all once in an auxiliary function, probably more efficient\n",
        "      # However, I would take the longest tweet, here I take only the longest in this batch.\n",
        "\n",
        "        if stop:\n",
        "          break;\n",
        "\n",
        "        # Update the start index for positive data and negative data\n",
        "\n",
        "        pos_index += n_to_take\n",
        "        neg_index += n_to_take\n",
        "\n",
        "        # Maximum length\n",
        "\n",
        "        # print(batch)\n",
        "    # return\n",
        "        max_len = max([len(tweet) for tweet in batch])\n",
        "\n",
        "        tensor_pad_list = []\n",
        "\n",
        "        for tensor in batch:\n",
        "            n_pad = max_len - len(tensor)\n",
        "\n",
        "            pad_l = [0] * n_pad\n",
        "            tensor_pad = tensor + pad_l\n",
        "\n",
        "            tensor_pad_list.append(tensor_pad)\n",
        "\n",
        "        tensors = np.array(tensor_pad_list) # This has positive and negative tweets\n",
        "\n",
        "        # Add labels\n",
        "        target_pos = [1] * n_to_take\n",
        "        target_neg = [0] * n_to_take\n",
        "\n",
        "        labels = target_pos + target_neg\n",
        "\n",
        "        labels = np.array(labels)\n",
        "\n",
        "        example_weights = np.ones_like(labels)\n",
        "\n",
        "    yield tensors, labels, example_weights"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "WEcQHM6ykhA2",
        "outputId": "8722d29c-4720-49d0-a8b3-1092dd3a4b16"
      },
      "source": [
        "# Test it\n",
        "\n",
        "def train_generator(batch_size, shuffle = False):\n",
        "    return data_generator(train_pos, train_neg, batch_size, True, vocab, shuffle)\n",
        "\n",
        "def test_generator(batch_size, shuffle = False):\n",
        "    return data_generator(test_pos, test_neg, batch_size, True, vocab, shuffle)\n",
        "\n",
        "def generate_one(batch_size, shuffle = False):\n",
        "    return data_generator(test_pos, test_neg, batch_size, False, vocab, shuffle)\n",
        "\n",
        "batch_size = 4\n",
        "rnd.seed(30)\n",
        "\n",
        "inputs, targets, example_weights = next(train_generator(batch_size), True)\n",
        "\n",
        "print(f'Inputs: {inputs}')\n",
        "print(f'Targets: {targets}')\n",
        "print(f'Example Weights: {example_weights}')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-e7e4c7ee1e36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Inputs: {inputs}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-74abb7a99c13>\u001b[0m in \u001b[0;36mdata_generator\u001b[0;34m(data_pos, data_neg, batch_size, loop, vocab_dict, shuffle)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;31m#Get the tweets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mtweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_pos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos_index_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Beware of the slicing, pos_index list is a list!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweet_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mpos_index\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-90b946537e7d>\u001b[0m in \u001b[0;36mtweet_to_tensor\u001b[0;34m(tweet, vocab_dict, unk_token)\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0mtensor_l\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mvector\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mindices\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \"\"\"\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mword_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_tweet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0munk_ID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munk_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/utils.py\u001b[0m in \u001b[0;36mprocess_tweet\u001b[0;34m(tweet)\u001b[0m\n\u001b[1;32m     48\u001b[0m             word not in string.punctuation): # remove punctuation\n\u001b[1;32m     49\u001b[0m             \u001b[0;31m#tweets_clean.append(word)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mstem_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# stemming word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mtweets_clean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;31m### END CODE HERE ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/stem/porter.py\u001b[0m in \u001b[0;36mstem\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step1c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step5a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/stem/porter.py\u001b[0m in \u001b[0;36m_step3\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    538\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0;34m'ical'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ic'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_positive_measure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0;34m'ful'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_positive_measure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0;34m'ness'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_positive_measure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m         ])\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/stem/porter.py\u001b[0m in \u001b[0;36m_apply_rule_list\u001b[0;34m(self, word, rules)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \"\"\"\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0msuffix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplacement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcondition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msuffix\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'*d'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ends_double_consonant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                 \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}